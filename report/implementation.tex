\subsection{Toolkit introduction}
4 components, what should it do (online/offline), image2image, gal2gal, cat2cat

\begin{figure}[htb]
  \centering
  \includegraphics[width=1\linewidth]{img/components.pdf}
  \caption{Interaction between the four components of the toolkit.}
  \label{fig:components}
\end{figure}

% toolboxes used
\subsection{Data collection}
% xml format
% network

The data collection is the first component in our toolkit.
It deals with downloading information and galleries from the deviantART website,
although the component can be replaced to deal with different art or picture related community websites. 

DeviantArt does not provide a web api to download images. This makes it more 
difficult to download galleries from the users. 
On top of that changes to the website can possible break the functionality of the component.

We followed the backend links of the galleries to the RSS XML files to access
the image data. Our \textit{gallery scraper} is implemented in python and uses
built-in libraries to parse the XML files. For each image we stored general 
information like the category, deviantART link and filename and we downloaded
the full image and the thumbnails.

For the network information collection we parsed the HTML watchers pages
of the users, since no RSS XML files are provided by deviantART for this
information.

\subsection{Feature extraction}
When working with images, it is usually not possible to work with the raw image data (the pixel values). The reason for this is the high dimensionality of images, which can easily exist in a space of more than a million dimensions. By extracting features from images, they can be represented in a lower dimensional feature-space.  This feature extraction process has several advantages:
\begin{itemize}
\item The data becomes computationally easier to work with due to the smaller number of dimensions
\item By using the right features, the data becomes more suitable for generalization across images
\item Reducing the dimensionality makes it easier to visualize sets of images
\item Features can have an intuitive basis, which makes it easier for non-computer-scientists to analyze (sets of) images
\end{itemize}

% Computer vision is an important and maturing engineering science. It underpins an increasing variety of applications that require the acquisition, analysis, and interpretation of visual information.
In the extraction of image features, a distinction was made between low-level statistical features and higher level cognitive-based features....

\subsubsection{Statistical features}
As statistical features, many relatively simple low-level features were extracted from the images.
The first type of statistical features that were used are color-based features, which should capture the color-usage in the artwork. Many artists produce collections of art pieces with similar colors, and should therefore be (partially) distinguishable with color-based features. For each of the three RGB channels, an average and median is calculated over all the channel values. The image is also converted into the HSV color space, from which the average and median Hue, Saturation and Value are extracted.  

The second group of features is the edge to pixel and corner to pixel ratio. The edge to pixel ratio is calculated by performing Canny edge detection on the image to construct an image of edges. The number of edge pixels in this image, divided by the total number of pixels is then used as a feature. The same is done using a corner detector. These two features should be helpful in distinguishing many photography artwork from other genres such as cartoons and manga. The latter two tend to have large patches of plain color patches, which will decrease the amount of edges and corners. They are also somewhat indicative to the type of scenes in photography. A blue sky will not produce many edges or corners, whereas a busy street will.  

For the final group of features, the artworks are converted to an intensity image. First of all, the average and median value of this intensity image is extracted. These values give information about the lightness or darkness of art. The second intensity feature is the variance, which reacts to the contrast between lightness and darkness in images. The final feature that is extracted from the intensity image is the entropy. The entropy is a statistical measure of randomness that somewhat characterizes the texture in an image.  

The features described above only contain global information about images. In order to capture localized information as well, several of the features described above are also extracted from different regions of the image. The regions of the image are obtained by dividing the image along both dimensions into NxM equal-sized regions. Since feature values will most likely vary from region region, these compositional features should provide valuable additional information about an image. 




%\begin{itemize}
%\item Edge ratio: description
%\item Corner ratio: description
%\item etc...
%\end{itemize}
\subsubsection{Weibull, this subsubsubsection can be made normal text, but we can do that later}
The contrast of natural image statistics has been shown to conform to Weibull-shaped probability distributions \cite{Weibull_physical}. Furthermore, when images do not adhere to this distribution, the images in question are mashups of multiple sub-images which themselves do conform to the Weibull distribution. In addition to this property of natural images, it has been proposed that the parameters for the Weibull distribution form a basis for the description of texture in images \cite{Weibull_6}. There is indeed evidence that the human visual system is capable of approximating the parameters of the Weibull distribution \cite{Weibull_brain}. Last the two most important parameters of the Weibull distribution, when it comes to natural images have a straight forward interpretation, the shape parameter the describes the resemblance to other probability distribution, from a power-law to the normal distribution, where the scale describes the how wide the distribution is. Therefore we included the maximum likelihood estimation of the Weibull-distribution for contrast of the image as used for \cite{Weibull_6} in our Feature extraction toolbox, unfortunately this seemed to give unstable results, therefore we later eliminated it. 

\subsubsection{Cognitively-inspired features}
As it has already been explained in the previous work, a saliency map enables the visual system to integrate large amounts of information. Til now those information has been used in scene understanding and object recognition. In this reasearch, image features have extracted from the map and then used in the classification and visualization task.  
It has been used the Itti's model \cite{Itti_model} due to its low computentional time and the existense of a free toolkit \footnote{http://www.saliencytoolbox.net/} made by Dirk Walther.
The model work as follow: an input image is decomposed through several pre-attentive feature detection mechanisms which operate in parallel channels over the entire visual scene, and four conspicuity maps (color, orientation, intensity and skin) are created. After few different intermediate steps, the model finally combines the four conspicuity maps into a unique saliency map. 
The toolkit has been used to create the saliency maps and the intermediate maps (color, intensity, orientation and skin). Features that have been extracted from those maps are: \textit{Shannon entropy} of the five maps, \textit{Standard deviation} of the distribution of attention in the saliency map, \textit{Location} of the most salient points (defined as the centers of the most salient regions) and \textit{Skin intensity} of the skin map. Skin is not a default channel in the Itti's model, but it has been found out to be really interesting and useful in devianART to distinguish artists and artworks, where there is a huge presence of photographer that create nude art. 

\subsection{Classification}
[Quang]
% which classifiers
% how they work
% how they operate in the system

\subsection{Visualization}
The final component of the toolkit is the visualization application.
This visualization is used to present the information that has been gathered by the other components of the toolkit.
The collected images are used together with the extracted image features (Fig.~\ref{fig:components}) to visualize the dataset.
This provides an effective way to find patterns in the dataset, analyze classification results and filter information.
The visualization application combines three different visualization techniques into one application.
The large blue, purple and brown buttons in the header of the application are used to switch between these visualization techniques.
Each visualization technique offers a different look on the dataset.

There are multiple data visualization applications and toolkits~\footnote{Wikiviz \url{http://www.wikiviz.org/wiki/Tools}} that are able to create visualizations out of the box.
But these applications and toolkits offer only generic displays and interactions, which do not capture the dataset in its full potential.
This disadvantage convinced us to create our own application.
The visualization application is written in the Java programming language.
The open source Processing API \footnote{\url{http://www.processing.org}} is used to draw all three visualizations.
The Processing API contains classes and functions that simplify drawing, animations and interactions in Java.
Processing was an obvious choice, because it has the right combination of cost, ease of use and speed~\cite{fry08}.

\begin{figure}[htb]
  \centering
  \includegraphics[width=1\linewidth]{img/visualization_scatter.png}
  \caption{The visualization application displaying a scatter plot of 2 artists.}
  \label{fig:visualization_scatter}
\end{figure}

Fig.~\ref{fig:visualization_scatter} shows the \textit{scatter plot} visualization technique.
The scatter plot displays values for two variables, which are image features that has been computed by the \textit{Feature extraction} component.
The data is displayed as a collection of miniature images.
Each having the value of one feature determining the position on the horizontal axis and the value of the other feature determining the position on the vertical axis.
The border around each image represents the class (artists or category) to which an image belongs.
For example, the images with a green border belong to the artist Kitsunebaka91 and the images with a red pink border belong to the artist Woekan.
The user has full control over which classes are displayed in the visualization.
The users can also control which two features are used as variables on the horizontal and vertical axis of the scatter plot.
A single image or all the images belonging to one class can be highlighted, making it more easy to recognize patterns.
The full version of a miniature image can be displayed to inspect it in more detail.

\begin{figure}[htb]
  \centering
  \includegraphics[width=1\linewidth]{img/visualization_parallel.png}
  \caption{The visualization application displaying a parallel coordinates plot of 3 artists and 5 features.}
  \label{fig:visualization_parallel}
\end{figure}

The scatter plot is limited to displaying only two features at the same time.
Fig.~\ref{fig:visualization_parallel} shows the \textit{parallel coordinates} visualization technique~\cite{andrienko2001constructing}, a common way of visualizing high-dimensional.
This enabled us to visualize beyond two features at the same time.
The two axises of the scatter plot are now replaced by n vertical parallel lines to represent n features (n-dimensional space).
An image is now represented as a polyline with vertices on the parallel axes.
The color of a polyline represents the class (artist, category) to which an image belongs.

\begin{figure}[htb]
  \centering
  \includegraphics[width=1\linewidth]{img/visualization_radial.png}
  \caption{DEMO IMG - ARTIFICIAL DATA - The visualization application displaying a radial plot that expresses the performance of the classification.}
  \label{fig:visualization_radial}
\end{figure}

Fig.~\ref{fig:visualization_radial} shows the \textit{radial plot} visualization technique.
This visualization is not used to visualize the dataset, but to display the performance of the classification, e.g., how a certain feature performs on separating an artist from the other artists.
The circle is split into three regions (variables): artists or categories (purple), features (yellow), classifiers (green).
%Each of these regions represent a variable that plays an important role in the classification of images.
%Each variables consists of a limited number of values that were used during classification.
The thickness of a line between two nodes expresses the performance of the classification when both nodes are used together, i.e., thicker lines means higher performance.
For example, a thick line between artist X and feature Y, means that feature Y is a good feature to separate artist X from the other artists.