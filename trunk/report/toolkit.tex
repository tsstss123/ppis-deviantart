% toolboxes used
\subsection{1. Data collection}
[Sander, Bart B.]
% xml format
% network

DeviantArt provides users, also called deviants, galleries of their images. 
When someone visits such a gallery a featured page will be shown. Furthermore
the user is provided with options to browse the gallery or visit a sub gallery
defined by the user. There are various types of members: normal members, premium 
members, banned members, staff members, etc. Premium members have extra benefits
like no ads, gallery customization, beta-test new site features and more.

DeviantArt does not provide a web api to download images. This makes it more 
difficult to download images. On top of that changes to the website can possible break
down the downloading application.

DeviantArt does provide rss, which allows us to download xml files containing 
information about the users galleries. RSS xml files are more easy to parse
than the html gallery pages.

For each image the full sized image and two different sized thumbnails are 
available. DeviantArt supports png, jpeg, bmp and gif image formats.

% gallery/images
% Initial approach
% 1. Retrieve backend url from http://deviant.deviantart.com/gallery (using the SMGL Parser included with python)
% 2. Retrieve the xml file from the backend (Using python xml.sax parser)
% 3. Parse all items from the xml file. Follow the links to the full images and thumbsnail. Download them.
% 4. Increase the offset parameter and parse the next xml file to retrieve all images from the deviant. Keep doing this until no items are left
% 5. The following file structure is created (add example):
%      - For each deviant a seperate folder is created. In this folder another three folders are created for the full sized images and the big and small thumbnails.
%      - For each image of the deviant a xml file is written containing the filename, deviantart link and title.
% Problems
% - The backend link in gallery page doesn't seems to have all images (only the ones in the features pages).
% - Downloading of images might fail due problems on the server side
% Approach refined
% - Instead of retrieving the backend url from the gallery page follow this link directly:
%    http://backend.deviantart.com/rss.xml?q=gallery:$deviant$&offset=0
%   Don't safe the xml file for the failed images. Remove corrupted images. Try to download again at a later point.
In order to create our dataset we needed to download the images from Deviantart. 
The \textit{Gallery Scraper} is written in Python and retrieves all images for a given 
list of deviants. In our initial approach we retrieved 
the backend link from \textit{http://deviant.deviantart.com/gallery} using the 
SGML(Standard Generalized Mark-up Language) parser included in the sgmllib Python module. The backend link points 
to a xml file containing the first 60 images of the deviant. TO retrieve the other 
xml files we needed to change the offset in the url. 

We parsed the xml files using the xml.sax module included with Python. 
For each item in the xml file a link to the full sized image, a big thumbnail and 
a small thumbnail are provided. The big thumbnails are usually around 300 width and 
the small thumbnails around 150 height. For each deviant we created a seperate folder, which
contains three subfolders for the different sized images. 

For each image of the deviant a xml file is written containing the filename, category, deviantart link and title.
An example of such a xml file looks like this:

(Insert example here)

Although this initial approach seems to work we did encountered a few problems. First it
seems the backend link on the gallery page only contains the images listed under \textit{featured}, 
which does not necessarily contains all images of the deviant. After some more investigation it turns 
out we can retrieve the xml file containing all images directly from the url 
\textit{$http://backend.deviantart.com/rss.xml?q=gallery:\$deviant\$$}.

The second problem is that downloading an image might fail due problems at the server side of DeviantArt.
We solved this by removing corrupt downloads and not generating a xml file for the image. Then at a later
point we run the scraper and try to download the missing images again. 

% quick analysis tool
%\subsubsection{Quick Analysis Tool}
% Prints category statistics per user or over the whole dataset
% Option to print sub categories
% Example print
To retrieve statistics about the dataset we made a quick analysis tool. It reads in
all image xml files using the xml.minidom Python module  and counts the categories. 
It has an option to this per user or for the whole dataset. It also has an option
to only print the top category statistics or also print per sub category.

An analysis including the sub categories looks like this:

(Insert example here)

\subsection{2. Feature extraction}
When working with images, it is usually not possible to work with the raw image data (the pixel values). The reason for this is the high dimensionality of images, which can easily exist in a space of more than a million dimensions. By extracting features from images, they can be represented in a lower dimensional feature-space.  This feature extraction process has several advantages:
\begin{itemize}
\item The data becomes computationally easier to work with due to the smaller number of dimensions
\item By using the right features, the data becomes more suitable for generalization across images
\item Reducing the dimensionality makes it easier to visualize sets of images
\item Features can have an intuitive basis, which makes it easier for non-computer-scientists to analyze (sets of) images
\end{itemize}
\textit{Here something general about different kinds of image features}

% Computer vision is an important and maturing engineering science. It underpins an increasing variety of applications that require the acquisition, analysis, and interpretation of visual information.
In the extraction of image features, a distinction was made between low-level statistical features and higher level cognitive-based features....

\subsubsection{Statistical features}
As statistical features, many relatively simple low-level features were extracted from the images. Below is a list of all features and their meaning:
\begin{itemize}
\item Edge ratio: description
\item Corner ratio: description
\item etc...
\end{itemize}

\subsubsection{Cognitively-inspired features}
One of the more recent trends in computer vision research in the pursuit of human-like capability is the coupling of cognition and vision into cognitive computer vision. 
The term cognitive computer vision has been introduced with the aim of achieving more robust, resilient, and adaptable computer vision systems by endowing them with a cognitive faculty: the ability to learn, adapt, weigh alternative solutions, and develop new strategies for analysis and interpretation.

In our project we focus on computational models of focal visual attention. Attention allows us to break down the problem of understanding a visual scene into a rapid series of computationally less demanding, localized visual analysis problems. 
"Visually salient" are those location in the visual wolrd that automatically attract attention.

\subsection{3. Classification}
An important aspect of analyzing images is to determine what makes them distinct.
In this tookit, classification is used to compare image sets and extract the image features that best separates them.
This knowledge can be used to describe the art style of an artist or even determine if there is an artist that uses an unique style.
Also classification can be extended to other category level to determine what the image features of a category are.

The classification part of the toolkit is implemented using the Matlab environment. In total there are four different classifiers that be called upon.

\begin{itemize}
	\item \textbf{k-Nearest Neighbour}, [DESC]
	\item \textbf{Naive Bayes}, [DESC]
	\item \textbf{Nearest Mean}, [DESC]
	\item \textbf{Support Vector Machine}, [DESC]
\end{itemize}

The classifiers were implemented using PRTools[REF] and libSVM[REF], which are existing implementations of the classifiers.

Before the classifiers can be used to find image features, several pre-processing steps on the dataset and the classifiers needs to be taken.

Naturally, functions that read in feature XML files, convert it to a dataset and splitting it into a train and test set are incorporated.
Also Min/Max normalization can be used on the dataset.
Normalization is important because the features that were extracted all uses different ranges of values, by scaling these values to the same range(e.g. [-0.5,0.5]) the classifiers are more effective in their task to find image features that separates an artist.

Furthermore filtering on classes and features can be performed on the dataset, this is to provide flexibility in a way that the user can do classification on just a few interesting classes or features instead of everything that is inside the dataset.

Another important aspect of classification is to do parameter optimization. 
Optimization of parameters will lead to better classification results because the classifier is more fine tuned to the type of task that lies ahead. 
The way the optimization is presented in this toolkit is by cross-validation on the trainset. 
For every parameter in the classifier cross-validation is performed on the trainset to compute the F-measure score. 
At the end the F-measure score of every parameter combination is compared and the parameter which yields the highest average F-measure score is the parameter which optimizes the classifier.

% which classifiers
% how they work
% how they operate in the system
\subsection{4. Visualization}
[Nick]
% 3 views
