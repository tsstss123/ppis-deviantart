\subsection{Network experiments}
The network experiments have been performed to analyze the dA network and establish global characteristics of the network, but also to find interesting sub-networks of artists and artworks which are of particular interest to investigate.

The dA network consists of 13 million registered users, doing a full analysis of the the whole network was impossible within the scope of the project. Therefore a network of the dA watchers functionality\footnote{a watcher is someone who indicates he likes an artist and would like to receive updates when that artist produces new artwork.} was extracted for all but casual artists, these latter can be distinguished with a $~$, indicating that they do not have a paid membership. This network formed the basis of our network experiments.

From this large network of around 100'000 professional artists three core-networks were extracted using the findcore algorithm, described in Algorithm \ref{alg:findcore}. The three cores correspond to three types of the $degree(j)$ function in that algorithm, namely - in-degree, out-degree and in+out-degree -. This has resulted in a core of artists that are "power-watchers" a core with "popular-watched" artists and last a mix of both.

The results of the network experiments are presented in Table \ref{tab:netstatistics}, Table \ref{tabnetcooccurrences} as well as Figure \ref{fig:results_core}.  For all networks statistics were extracted, unfortunately for the large professional users network statistics such as the characteristic path length were not possible to compute within the given time period.

\begin{table}[htb]
    \centering
    \begin{tabular}
        { | l | c | c | c | c | c | c|} 
        \hline
        Network &  prof.  & watchers & watched & mixed\\
	            &  artists & core & core & core \\
        \hline
	no. nodes $(n)$& 103'663 & 1'701 & 1471 & 1099 \\
	no. links & 4'483'023 & 139'285 & 127'837  & 166'244 \\
	avg. degree $(k)$& 43.25 & 81.88 & 86.90 & 151.27 \\
	findcore $\max x$ & & 43 & 44 & 185\\
	$L_G$ & - & 2.15 & 2.27 & 2.14 \\
	$L_{random}$ & - & 1.69 & 1.63 & 1.40 \\
	$C_G$ & - & 0.20 & 0.22 & 0.20 \\
	$C_{random}$ & - & 0.048 & 0.059 & 0.140 \\
	\hline
    \end{tabular}
    \caption{Network statistics}
    \label{tab:netstatistics}
\end{table}
\begin{table}[htb]
    \centering
    \begin{tabular}
        { | l | c | c | c |} 
        \hline
        core Network & watchers & watched & mixed\\
        co-occurrences  &   core & core & core \\
        \hline
        watchers core & 1701 &541& 54\\
        watched core & 541 &1471& 286\\
        mixed core  &54 &286 &1099\\
	\hline
    \end{tabular}
    \caption{Core network co-occurrences, there are 14 artists which are in all 3 core networks}
    \label{tab:netcooccurrences}
\end{table}

\begin{figure*}[htb]
  \centering
  \includegraphics[width=1\linewidth]{img/core.png}
  \caption{Core networks, the degree of watching versus being watched.  The green dots represent the whole professional network, while the blue represents the watchers-core, red the watched and pink the mixed core, the last sub figure shows the power-law behavior of the degree distributions.}
  \label{fig:results_core}
\end{figure*}

Through application of the findcore algorithm we have been able to find cores of interesting users, which are of manageable size for full analysis. To establish if these core networks are small-world, we refer to the definition of small-world networks as described in section \ref{net_proposed_approach}, a network can be considered small-world when $L_G\approx L_{random}$ and $C_G \gg C_{random}$. All core networks have a clustering coefficient larger than a comparable random network, though the mixed core is close to the random network. The characteristic path lengths are longer than their random network equivalents. This is most likely due to the relatively small size of the networks, the paths are still significantly smaller than $L_{lattice}=\frac{n}{2k}$. We therefore conclude that the core's of dA can be considered small-world networks.  
Furthermore the co-occurrences between the watchers and watched network is about $\frac{1}{3}$, while this is much lower for the mixed network. We presume that this has to do with the findcore degree $\max x$ which is much higher for the mixed network. Thus the mixed network is distinct from the watchers and watched cores which are more alike.



\subsection{Image experiments}
For the image experiments we collected our dataset by downloading the complete galleries from around 30 artists.
Those artists were selected from the daily deviations of a random day and includes both premium and non premium members.
In total we downloaded around 5000 images. 
For each image we also stored the category information.
The dataset is unbalanced since some artists only have around 50 images, while other artists have over 500 images.
The top five categories are listed in table \ref{datasetstats}.


\begin{table}[htb]
    \centering
    \begin{tabular}
        { | l | c | l | c|} 
        \hline
        Name & count & Name & count \\
        \hline
    Craniata &  61 & K1lgore & 63 \\
    Kitsunebaka91 & 272  & Knuxtiger4 & 193 \\
    LALAax & 82  & Mallimaakari & 151 \\
    Mentosik8 & 110 & NEDxfullMOon & 116 \\
    One-Vox & 74 & Pierrebfoto & 188 \\
    Red-Priest-Usada & 132 & Skarbog & 229 \\
    Swezzels & 9 & Udodelig & 54 \\
    UdonNodu & 38 & WarrenLouw & 51 \\
  	erroid & 146 & fediaFedia & 303 \\
  	gsphoto & 661 & iakobos & 47 \\
  	kamilsmala & 39 & miss-mosh & 307 \\
  	nyctopterus & 68 & omega300m & 483 \\
  	sekcyjny & 55 & stereoflow & 166\\
  	sujawoto & 33 & wirestyle & 143\\
  	woekan & 49 & zihnisinir & 177 \\

        \hline 
    \end{tabular}
    \caption{Dataset artist statistics}
    \label{datasetstats2}
\end{table}

\begin{figure}[htb]
%  \centering
  \includegraphics[width=1\linewidth]{img/datasetAvg.png}
  \caption{mean image per artist, left to right, top to bottom, in order of Table \ref{datasetstats2}: Craniata, K1lgore, Kitsunebaka91, ..., zihnisinir }
  \label{fig:avgDataset}
\end{figure}


\begin{table}[ht]
	\centering
	\subtable[]{
	
    \begin{tabular}
        { | l | c | } 
        \hline
        Category & count \\
        \hline
        photography & 2244 \\ 
        customization & 906 \\ 
        traditional & 842 \\ 
        digitalart & 587 \\ 
        fanart & 239 \\ 
        \hline 
    \end{tabular}
	}
	
	\subtable[]{
	\begin{tabular} { |l|c| }
		\hline
		Total Size & 1.4gb \\
		\hline
		Users & 31 \\
		\hline
		Image count & 5324 \\
		\hline 
		Average image width & 671 \\
		\hline
		Average image height & 689 \\
		\hline
		Average image size & 250kb \\
		\hline
		Average image count & 171 \\
		\hline
	\end{tabular}
	}
    \caption{Dataset category statistics}
    \label{datasetstats}
\end{table}

%\subsubsection{Experiment 1: One against all artists}

%The first experiment using this toolkit is to find out if there are any artists in our dataset that uses an unique style.
%The experiment is performed using two feature combination, the statistical features and the cognitive features.
%Also it is focused on using one artist as the positive class while all the other artists are in the negative class resulting in a two class problem.
%This experiment is set up as an orientation experiment to find out what might work for this dataset and what might not.
%Also this experiment can give a clear indication of how to approach this type of dataset.
%
%For this experiment, three classifiers were used: kNN, Naive Bayes and the Nearest Mean classifiers.
%The classifiers were trained on a train set which consists of 70\% of the entire dataset.
%Training was done with a 5 fold cross-validation and the performance of each classifier can be computed by calculating the average of the $F_1$-measure gathered from all folds.
%Because this is an orientation experiment, no test set was incorporated in this experiment.
%
%%\subsubsection{Experiment 1: Results}
%
%%\begin{table*}
%%    \centering
%%    \begin{tabular}
%%        { | l | l | c | c |} 
%%        \hline
%%        Artist & Classifier & Parameter & F-Measure \\
%%        \hline
%%        my\_shots & Nearest Mean & - & 0.8742 \\ 
%%        gsphoto & kNN & 3 & 0.8722 \\ 
%%        gsphoto & kNN & 5 & 0.8711 \\ 
%%        Pierrebfoto & Naive Bayes & 10 & 0.8698 \\ 
%%        Kitsunebaka91 & Naive Bayes & 24 & 0.8693 \\
%%        Kitsunebaka91 & Naive Bayes & 12 & 0.8680 \\
%%        gsphoto & kNN & 7 & 0.8669 \\
%%        Kitsunebaka91 & Naive Bayes & 22 & 0.8668 \\
%%        gsphoto & kNN & 9 & 0.8664 \\
%%        Kitsunebaka91 & Naive Bayes & 8 & 0.8657 \\
%%        \hline 
%%    \end{tabular}
%%    \caption{Top 10 Ranking of the highest $F_1$-measure using \textit{statistical features}}
%%    \label{ex1aresults}
%%\end{table*}
%
%\begin{table}[htb]
%    \centering
%    \begin{tabular}
%        { | l | l | c |} 
%        \hline
%        Artist & Classifier & $F_1$-Measure \\
%        \hline
%        my\_shots & Nearest Mean & 0.8742 \\ 
%        gsphoto & kNN & 0.8722 \\ 
%        gsphoto & kNN & 0.8711 \\ 
%        Pierrebfoto & Naive Bayes & 0.8698 \\ 
%        Kitsunebaka91 & Naive Bayes & 0.8693 \\
%        Kitsunebaka91 & Naive Bayes & 0.8680 \\
%        gsphoto & kNN & 0.8669 \\
%        Kitsunebaka91 & Naive Bayes & 0.8668 \\
%        gsphoto & kNN & 0.8664 \\
%        Kitsunebaka91 & Naive Bayes & 0.8657 \\
%        \hline 
%    \end{tabular}
%    \caption{Top 10 Ranking of the highest $F_1$-measure using \textit{statistical features}}
%    \label{ex1aresults}
%\end{table}
%
%The results from the experiment for the statistical features can be found in table \ref{ex1aresults}.
%The table lists the top 10 ranking of the artist names combined with the classifier and parameter used to train the classifier.
%The ranking is in a descending order starting from the combination that performed the best according to the $F_1$-measure.
%
%The table shows that 2 artist names exists multiple times in the top 10 ranking.
%Those artists are \textit{Kitsunebaka91} and \textit{gsphoto} and the table shows that they were separated from all other artists using different classifiers and parameter settings.
%
%The table also shows that the nearest mean classifier only has one entry in the top 10.
%Eventhough it has the highest $F_1$-measure, the classifier does not seem to be performing well on the other artists.
%
%%\begin{table*}
%%    \centering
%%    \begin{tabular}
%%        { | l | l | c | c |} 
%%        \hline
%%        Artist & Classifier & Parameter & F-Measure \\
%%        \hline
%%        giannisgx89 & Nearest Mean & - & 0.8549 \\ 
%%        gsphoto & Nearest Mean & - & 0.8196 \\ 
%%        Pierrebfoto & Nearest Mean & - & 0.7305 \\ 
%%        sekcyjny & Nearest Mean & - & 0.7298 \\ 
%%        fediaFedia & Nearest Mean & - & 0.7133 \\
%%        NEDxfullMOom & Nearest Mean & - & 0.7020 \\
%%        erroid & Nearest Mean & - & 0.6831 \\
%%        gsphoto & Naive Bayes & 14 & 0.6820 \\
%%        Mentosik8 & Nearest Mean & - & 0.6801 \\
%%        gsphoto & Naive Bayes & 10 & 0.6753 \\
%%        \hline 
%%    \end{tabular}
%%    \caption{Top 10 Ranking of the highest $F_1$-measure using \textit{cognitive} features}
%%    \label{ex1bresults}
%%\end{table*}
%
%\begin{table}[htb]
%    \centering
%    \begin{tabular}
%        { | l | l | c |} 
%        \hline
%        Artist & Classifier & $F_1$-Measure \\
%        \hline
%        giannisgx89 & Nearest Mean & 0.8549 \\ 
%        gsphoto & Nearest Mean & 0.8196 \\ 
%        Pierrebfoto & Nearest Mean & 0.7305 \\ 
%        sekcyjny & Nearest Mean & 0.7298 \\ 
%        fediaFedia & Nearest Mean & 0.7133 \\
%        NEDxfullMOom & Nearest Mean & 0.7020 \\
%        erroid & Nearest Mean & 0.6831 \\
%        gsphoto & Naive Bayes & 0.6820 \\
%        Mentosik8 & Nearest Mean & 0.6801 \\
%        gsphoto & Naive Bayes & 0.6753 \\
%        \hline 
%    \end{tabular}
%    \caption{Top 10 Ranking of the highest $F_1$-measure using \textit{cognitive} features}
%    \label{ex1bresults}
%\end{table}
%
%Table \ref{ex1bresults} shows the results obtained from only using cognitive features to separate artists from each other.
%In contrast to table \ref{ex1aresults} these results show that the nearest mean classifier is the one having the best performance according to $F_1$-measure.
%
%However using cognitive features results into a different ranking of separable artists, the performance of each classifier is not as high as for using statistical features.
%The top 10 ranking using the statistical features all have scores above the 0.8 indicating that using statistical features seems more stable compared to using cognitive features.
%
%This orientation experiment showed that using statistical features better results can be obtained compared to using cognitive features.
%Also it seems like there are some artists that can be seperable from all the others but most artists do not perform well in this experiment.
%Using the results of this experiment it is hard to describe why this is happening. 
%An experiment showing which features contributes to the separation of an artist from all other artists is important in order to describe why the style of an artist is unique compared to the other artists in the dataset.
%The next section describes an experiment that gives a clear view over these questions.

\subsubsection{Experimental Setup}

This experiment is focused on finding the most defining features of every artist in the dataset.
For every artist, the feature selection algorithm selects the best 5 features that according to the inter-intra criterion can separate that artist with another artist.
This will be done for every artist pair that can be found in the dataset, meaning that the best 5 features can differ depending on what kind of artist pair is used.

At first, a classifier needs to be selected which is going to be used to give a performance score for each set of features that the feature selection algorithm is computing.
The classifier is selected by optimizing the parameters for each classifier using 5 fold cross-validation on the training set, which is 70 percent of the dataset.
The classifier selected will be the one that gives the highest performance score.

For every artist pair a performance score is computed based on how well the selected classifier can separate those artists given the features.
Therefore the performance measure used here is the $F_1$-measure.

%For this experiment each artist is compared to one other artist.
%The motivation of this approach is that each artist can be evaluated separately and it can give more clarity for cases where the performance of a classifier is low.
%The experiment is done for all the artists in the dataset meaning that an artist is compared to every other artist in the dataset.
%Also this experiment uses the feature selection algorithm to determine the features that best separates an artist from another artist.
%The feature selection algorithm selects features up to a combination of 5 different features for every artist pair.
%
%For this experiment the kNN, Naive Bayes, Nearest Mean and the SVM classifiers were optimized and trained on the train set using 5 fold cross-validation and the highest average $F_1$-measure determines the classifier that is used to produce the results.
%
%For every artist pair the $F_1$-measure is calculated on the test set to evaluate the performance of the classifier separating those artists.

\subsubsection{Image Experiment Results}

\begin{table}[htb]
    \centering
    \begin{tabular}
        { | l | c | c |} 
        \hline
        Classifier & Mean $F_1$-measure & Stddev $F_1$-measure  \\
        \hline
        kNN & 0.7074 & 0.1731 \\ 
        Naive Bayes & 0.7897 & 0.1030 \\ 
        Nearest Mean & 0.7383 & 0.1086 \\ 
        Linear SVM & 0.8278 & 0.1450 \\ 
        \hline 
    \end{tabular}
    \caption{The mean $F_1$-measure and the standard deviation performance score of each optimized classifier on the train set using 5-fold cross-validation}
    \label{ex2optimizeresults}
\end{table}

The results of optimizing the classifiers and then validating them using the train set are shown in table \ref{ex2optimizeresults}.
The results show that the linear SVM has got the highest mean $F_1$-measure score and also the second highest standard deviation.
Based on these scores, it is best to select the linear SVM to assign performance scores because this classifier can best separate artists.

%Table \ref{ex2optimizeresults} shows the results After the optimization and training of the classifiers.
%The linear SVM classifier obtained the highest average $F_1$-measure however the Naive Bayes classifier did not perform significantly worse than the SVM.
%Note that the main goal of this experiment is to find feature combinations that distinguish artists.
%Therefore the focus should not go to maximizing the $F_1$-measure on the test set meaning that one classifier should be enough for this type of task.

\begin{figure*}[htb]
  \centering
  \includegraphics[width=0.75\linewidth]{img/experiment2results.png}
  \caption{The intensity mapping of the results using the linear SVM in combination with different sets of features.
  The mapping is divided into 30x30 squares where each square represents the $F_1$-measure of artist x with artist y.
  The artists are represented on both the x and the y axis by numbers.
  Table \ref{ex2stats} shows which artists are corresponding to which number.
  Also the red color represents the highest value($F_1$-measure=1).
  In the upperleft mapping, the performance using only \textit{the most} informative feature according to the selection algorithm is mapped.
  In the upperright mapping, the performance of using the \textit{3} most informative features are mapped.
  In the lowerleft mapping, the performance of using the \textit{5} most informative features are mapped.
  In the lowerright mapping, the performance of using \textit{all} the features are mapped.
  }
  \label{fig:experiment2results}
\end{figure*}

The results of using the trained linear SVM on the test set is shown in figure \ref{fig:experiment2results}.
The figure shows the intensity mappings of different lengths of feature sets.
It shows that the length of the feature sets used to separate artists is correlated to the performance score between them.
It also shows that there are artists pairs that can be separated using only a few features.
There are even artists which can be separated from every other artist in the dataset.
This is shown by the columns and rows that are predominantly red.
These artists uses an art style that is unique compared to the other artists in this dataset.
This result tells that those artists can be described using the features that were created using this toolkit.

\begin{table*}[htb]
    \centering
    \begin{tabular}
        { | c | l | c | c | l | } 
        \hline
        No. & Name & mean $F_1$-measure & stddev $F_1$-measure & Most defining features\\
        \hline
    1 & Craniata &  0.6314 & 0.1864 & Center average Hue, Center corner detection, Center edge detection\\
    2 & K1lgore & 0.6078 & 0.1813 & Entropy of the Intensity, Variance of the Intensity, Edge detection \\
    3 & Kitsunebaka91 & 0.8827 & 0.1756 & Center edge detection, Entropy of the Intensity, Edge detection \\
    4 & Knuxtiger4 & 0.7751 & 0.1822 & Variance of the Intensity, Entropy of the Intensity, Center edge detection \\
    5 & LALAax & 0.6498 & 0.1902 & Edgeratio, Entropy of the Intensity, Variance of the Intensity \\
    6 & Mallimaakari & 0.7548 & 0.1824 & Variance of the Intensity, Center edge detection, Center average Hue\\
    7 & Mentosik8 & 0.6807 & 0.1876 & Entropy of the Intensity, Variance of the Intensity, Lowerleft edge detection \\ 
    8 & NEDxfullMOon & 0.7071 & 0.1880 & Center edge detection, Center avg B, Entropy of the Intensity\\
    9 & One-Vox & 0.6570 & 0.1833 & Entropy of the Intensity, Edge detection, Center edge detection  \\
    10 & Pierrebfoto & 0.8375 & 0.1852 & salMapKEntropy, Center average Hue, Edge detection\\
    11 & Red-Priest-Usada & 0.7376 & 0.1851 & salMapIEntropy, Center Edge detection, salMapOEntropy\\
    12 & Skarbog & 0.7875 & 0.1869 & Entropy of the Intensity, Center average saturation, Lowerright average R \\
    13 & Swezzels & 0.3296 & 0.2281 & - \\ 
    14 & Udodelig & 0.5798 & 0.1887 & Entropy of the Saliency, Entropy of the Intensity, salhistdev \\
    15 & UdonNodu & 0.5352 & 0.2158 & SalmapOentropy, Variance of the Intensity, Uppermiddel average Hue \\ 
    16 & WarrenLouw & 0.6235 & 0.1864 & Entropy of the Intensity, median B, Upperright median Intensity\\
    17 & erroid & 0.7535 & 0.1833 & Variance of the Intensity, Center edge detection, Entropy of the Intensity\\
    18 & fediaFedia & 0.8270 & 0.1804 & Center edge detection, corner detection, Variance of the Intensity \\
    19 & gsphoto & 0.9172 & 0.1779 & Entropy of the Intensity, Variance of the Intensity, Center edge detection \\
    20 & iakobos & 0.5764 & 0.1899 & Edge detection, upperleft average saturation, upperleft median G \\
    21 & kamilsmala & 0.6359 & 0.1978 & Variance of the Intensity, Entropy of the Intensity, middleleft edge detection\\
    22 & miss-mosh & 0.8205 & 0.1811 & upperleft corner detection, Number of faces, Entropy of the Intensity \\
    23 & nyctopterus & 0.6324 & 0.1950 & Variance of the Intensity, Center median Hue, center average Hue\\
    24 & sekcyjny & 0.5875 & 0.1983 & Entropy of the Intensity, Downmiddle edge detection, salmapOEntropy\\
    25 & stereoflow & 0.7576 & 0.1904 & Center average Hue, Variance of the Intensity, Corner detection \\
    26 & sujawoto & 0.4752 & 0.1873 & SalmapIEntropy, Entropy of the Intensity, Downmiddle edge detection\\
    27 & wirestyle & 0.7138 & 0.1931 & SalmapIEntropy, Variance of the Intensity, Center average B\\ 
    28 & woekan & 0.5529 & 0.1881 & Entropy of the Intensity, Variance of the Intensity, Upperleft edge detection\\
    29 & zihnisinir & 0.7632 & 0.1918 & Entropy of the Intensity, Center average saturation, SalMapIEntropy\\
    30 & omega300m & 0.8318 & 0.2403 & Entropy of the Intensity, Center edge detection, Downright average Intensity \\
        \hline 
    \end{tabular}
    \caption{Dataset artist statistics}
    \label{ex2stats}
\end{table*}

%Figure \ref{fig:experiment2results} shows the results for this experiment using the linear SVM which was chosen in a greedy fashion based on the results found in table \ref{ex2optimizeresults}.
%From left to right the figure shows the results of using: \textit{1} feature, \textit{2} features, \textit{3} features, \textit{4} features, \textit{5} features and \textit{all} features as the feature combination that is used by the classifier to classify images.
%The results are plotted using an intensity mapping where each square represents the $F_1$-measure performance of separating the \textit{x} and \textit{y} artist.
%
%The first thing that is noticable is that the intensity mappings are not symmetric.
%This is because the $F_1$-measure was used here to evaluate the performance.
%The measure is calculated by looking at the precision and recall of the positive class, so when the classes are turned around the $F_1$-measure will also turn into a different value.
%Therefore in order to determine if two artists are separable, there is a need to look at the $F_1$-measure for both artists.
%This is because for some artist pairs the $F_1$-measure of one artist has a value of 1 while the other has a value of 0.
%This means that for those artist pairs the classifier has classified all images to one artist, which most of the time is caused by the non-uniform dataset.
%
%Furthermore the figure shows that artists can be better separated by using more features.
%The intensity mapping using all features shows that it has the highest overall $F_1$-measure.
%However the more interesting mappings are the ones where only the top 1,2,3,4,5 features were used for classification.
%Eventhough their performance is lower than using all the features it gives information about the most defining features that is used to separate an artist from another artist.
%
%The intensity mappings using 1 to 5 features contains artists that have an overall high $F_1$-measure which shows that some artists can be separated from all other artists using only a few features.
%
%\begin{table}[htb]
%    \centering
%    \begin{tabular}
%        { | l | c |} 
%        \hline
%        Feature & Counts \\
%        \hline
%        gridEdgeRatio\_5 & 9 \\ 
%        intEntropy & 4 \\ 
%        edgeRatio & 4 \\ 
%        avgRCells\_9 & 4 \\ 
%        medIntCells\_2 & 3 \\
%        medIntCells\_7 & 3 \\
%        \hline 
%    \end{tabular}
%    \caption{Counts of features that were used to separate \textit{Kitsunebaka91} from the other artists.}
%    \label{kitsune}
%\end{table}
%
%Table \ref{kitsune} shows the top 6 feature counts of features that were used to distinct \textit{Kitsunebaka91} from the other artists.
%The features were counted whenever the $F_1$-measure of \textit{Kitsunebaka91} had an $F_1$-measure higher than 0.9, which indicates that the artist is separable from the other artist.
%So the table details the reason why \textit{Kitsunebaka91} was separable.
%The artwork of \textit{Kitsunebaka91} consists mostly of images where there is a white background with a drawing in the center of it.
%This corresponds to the features found using this toolkit because table \ref{kitsune} shows that the edgeratio and intensity of the images are the dominant features to separate this artist from other artists.
%Also the table shows that the \textit{GridEdgeRatio\_5} feature is used multiple times to separate this artist.
%This again corresponds to the artist style because the \textit{GridEdgeRatio\_5} is the edgeratio value of the center cell of an image, the cell where the drawing of the artist lies.
%
%Eventhough \textit{Kitsunebaka91} was seperable from most other artists, pairing \textit{Kitsunebaka91} with the artist \textit{Knuxtiger4} resulted in a lower $F_1$-measure of 0.7397 wheras the mean of all the scores was 0.91.
%Inspecting the images of \textit{Knuxtiger4} showed that the artist mostly contained images that were drawings on a white background similar to that of \textit{Kitsunebaka91}.
%Therefore it is not surprising that the score for that pair was lower than the average because the classifier could not seperate those images.
%These results indicates that these results can be used to create a recommender system.
