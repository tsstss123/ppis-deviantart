\subsection{Toolkit introduction}
4 components, what should it do (online/offline), image2image, gal2gal, cat2cat

\begin{figure}[htb]
  \centering
  \includegraphics[width=1\linewidth]{img/components.pdf}
  \caption{Interaction between the four components of the toolkit.}
  \label{fig:components}
\end{figure}

\subsection{Data collection}
\subsection{Feature extraction}
\subsection{Classification}
\subsection{Visualization}

% toolboxes used
\subsection{Data collection}
[Sander, Bart B.]
% xml format
% network

DeviantArt provides users, also called deviants, galleries of their images. 
When someone visits such a gallery a featured page will be shown. Furthermore
the user is provided with options to browse the gallery or visit a sub gallery
defined by the user. There are various types of members: normal members, premium 
members, banned members, staff members, etc. Premium members have extra benefits
like no ads, gallery customization, beta-test new site features and more.

DeviantArt does not provide a web api to download images. This makes it more 
difficult to download images. On top of that changes to the website can possible break
down the downloading application.

DeviantArt does provide rss, which allows us to download xml files containing 
information about the users galleries. RSS XML files are more easy to parse
than the html gallery pages.

For each image the full sized image and two different sized thumbnails are 
available. DeviantArt supports png, jpeg, bmp and gif image formats.

For the data collection of the galleries we followed the backend links to
the RSS XML files. Our \textit{scraper} is implemented in python and uses
built-in libraries to parse the XML files. For each image we stored general 
information like the category, deviantART link and filename and we downloaded
the full image and the thumbnails.

For the network information collection we parsed the HTML watchers pages
of the users, since no RSS XML files are provided by deviantART for this
information.

\subsection{Feature extraction}
When working with images, it is usually not possible to work with the raw image data (the pixel values). The reason for this is the high dimensionality of images, which can easily exist in a space of more than a million dimensions. By extracting features from images, they can be represented in a lower dimensional feature-space.  This feature extraction process has several advantages:
\begin{itemize}
\item The data becomes computationally easier to work with due to the smaller number of dimensions
\item By using the right features, the data becomes more suitable for generalization across images
\item Reducing the dimensionality makes it easier to visualize sets of images
\item Features can have an intuitive basis, which makes it easier for non-computer-scientists to analyze (sets of) images
\end{itemize}
\textit{Here something general about different kinds of image features}

% Computer vision is an important and maturing engineering science. It underpins an increasing variety of applications that require the acquisition, analysis, and interpretation of visual information.
In the extraction of image features, a distinction was made between low-level statistical features and higher level cognitive-based features....

\subsubsection{Statistical features}
As statistical features, many relatively simple low-level features were extracted from the images.
The first type of statistical features that were used are color-based features, which should capture the color-usage in the artwork. Many artists produce collections of art pieces with similar colors, and should therefore be (partially) distinguishable with color-based features. For each of the three RGB channels, an average and median is calculated over all the channel values. The image is also converted into the HSV color space, from which the average and median Hue, Saturation and Value are extracted.  

The second group of features is the edge to pixel and corner to pixel ratio. The edge to pixel ratio is calculated by performing Canny edge detection on the image to construct an image of edges. The number of edge pixels in this image, divided by the total number of pixels is then used as a feature. The same is done using a corner detector. These two features should be helpful in distinguishing many photography artwork from other genres such as cartoons and manga. The latter two tend to have large patches of plain color patches, which will decrease the amount of edges and corners. They are also somewhat indicative to the type of scenes in photography. A blue sky will not produce many edges or corners, whereas a busy street will.  

For the final group of features, the artworks are converted to an intensity image. First of all, the average and median value of this intensity image is extracted. These values give information about the lightness or darkness of art. The second intensity feature is the variance, which reacts to the contrast between lightness and darkness in images. The final feature that is extracted from the intensity image is the entropy. The entropy is a statistical measure of randomness that somewhat characterizes the texture in an image.  

The features described above only contain global information about images. In order to capture localized information as well, several of the features described above are also extracted from different regions of the image. The regions of the image are obtained by dividing the image along both dimensions into NxM equal-sized regions. Since feature values will most likely vary from region region, these compositional features should provide valuable additional information about an image. 




%\begin{itemize}
%\item Edge ratio: description
%\item Corner ratio: description
%\item etc...
%\end{itemize}
\subsubsection{Weibull, this subsubsubsection can be made normal text, but we can do that later}
The contrast of natural image statistics has been shown to conform to Weibull-shaped probability distributions \cite{Weibull_physical}. Furthermore, when images do not adhere to this distribution, the images in question are mashups of multiple sub-images which themselves do conform to the Weibull distribution. In addition to this property of natural images, it has been proposed that the parameters for the Weibull distribution form a basis for the description of texture in images \cite{Weibull_6}. There is indeed evidence that the human visual system is capable of approximating the parameters of the Weibull distribution \cite{Weibull_brain}. Last the two most important parameters of the Weibull distribution, when it comes to natural images have a straight forward interpretation, the shape parameter the describes the resemblance to other probability distribution, from a power-law to the normal distribution, where the scale describes the how wide the distribution is. Therefore we included the maximum likelihood estimation of the Weibull-distribution for contrast of the image as used for \cite{Weibull_6} in our Feature extraction toolbox, unfortunately this seemed to give unstable results, therefore we later eliminated it. 

\subsubsection{Cognitively-inspired features}
One of the more recent trends in computer vision research in the pursuit of human-like capability is the coupling of cognition and vision into cognitive computer vision. 
The term cognitive computer vision has been introduced with the aim of achieving more robust, resilient, and adaptable computer vision systems by endowing them with a cognitive faculty: the ability to learn, adapt, weigh alternative solutions, and develop new strategies for analysis and interpretation.

In our project we focus on computational models of focal visual attention. Attention allows us to break down the problem of understanding a visual scene into a rapid series of computationally less demanding, localized visual analysis problems. 
"Visually salient" are those location in the visual wolrd that automatically attract attention.

\subsection{Classification}
[Quang]
% which classifiers
% how they work
% how they operate in the system

\subsection{Visualization}
The final component of the toolkit is the visualization application.
This visualization is used to present the information that has been gathered by the other components of the toolkit.
The collected images are used together with the extracted image features (Fig.~\ref{fig:components}) to visualize the dataset.
This provides an effective way to find patterns in the dataset, analyze classification results and filter information.
The visualization application combines three different visualization techniques into one application.
The large blue, purple and brown buttons in the header of the application are used to switch between these visualization techniques.
Each visualization technique offers a different look on the dataset.

\begin{figure}[htb]
  \centering
  \includegraphics[width=1\linewidth]{img/visualization_scatter.png}
  \caption{The visualization application displaying a scatter plot of 2 artists.}
  \label{fig:visualization_scatter}
\end{figure}

Fig.~\ref{fig:visualization_scatter} shows the \textit{scatter plot} visualization technique.
The scatter plot displays values for two variables, which are image features that has been computed by the \textit{Feature extraction} component.
The data is displayed as a collection of miniature images.
Each having the value of one feature determining the position on the horizontal axis and the value of the other feature determining the position on the vertical axis.
The border around each image represents the class (artists or category) to which an image belongs.
For example, the images with a green border belong to the artist Kitsunebaka91 and the images with a red pink border belong to the artist Woekan.
The user has full control over which classes are displayed in the visualization.
The users can also control which two features are used as variables on the horizontal and vertical axis of the scatter plot.
A single image or all the images belonging to one class can be highlighted, making it more easy to recognize patterns.
The full version of a miniature image can be displayed to inspect it in more detail.

\begin{figure}[htb]
  \centering
  \includegraphics[width=1\linewidth]{img/visualization_parallel.png}
  \caption{The visualization application displaying a parallel coordinates plot of 3 artists and 5 features.}
  \label{fig:visualization_parallel}
\end{figure}

The scatter plot is limited to displaying only two features at the same time.
Fig.~\ref{fig:visualization_parallel} shows the \textit{parallel coordinates} visualization technique~\cite{andrienko2001constructing}, a common way of visualizing high-dimensional.
This enabled us to visualize beyond two features at the same time.
The two axises of the scatter plot are now replaced by n vertical parallel lines to represent n features (n-dimensional space).
An image is now represented as a polyline with vertices on the parallel axes.
The color of a polyline represents the class (artist, category) to which an image belongs.