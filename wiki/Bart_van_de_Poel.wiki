#summary Bart's research log

=Research Log=

==09/06==
 * Found a parameters in the facedetect source code that affects how easily something is detected as a face (something about positive neighbours). But even when I set it relatively high it gives a lot of false positives. 
 * Created a function that returns the number of detected faces as a feature (might also want to give location). But if the detections are too noisy, we may want to remove this feature.
 * Created function that uses the opencv goodFeaturesForTracking function to detect corners and returns the number of corners in an image.

==08/06==
 * Wasted a couple of hours trying to get the opencv/matlab code that albert sent to work, but it won't compile correctly on mac. 
 * Finally works! Turned out to be a small compiler-dependent error in the source code. Tried canny edge, and rgb2hsv and they both were 10-50 times faster then the matlab versions.
 * Rewrote the feature extraction code so that the different feature codes just take one image as input and return the feature(vector) instead of being standalone pieces of code. And replaces builtin matlab functions with opencv functions where possible.
 * Tried the viola and jones face detector. Works, but haven't found out how to give parameters like the confidence threshold yet (otherwise it will give a lot of false positives). 

==07/06==
 * We all had a meeting with albert
 * Looked at using opencv from matlab

==06/06==
 * wrote extraction of avg hue, saturation and intensity

==04/06==
 * Meeting with team about research goals and specifics
 * Decided on how to output features and wrote an example feature extractor to extract the edge to non-edge ratio of an image (Note: need to decide on parameters for the canny edge detector). Output is a textfile with a line for each image with the format filename,ratio:
1980_by_K1lgore.jpg,0.069953

==03/06==
 * Looked for papers about statistical features
 * Read paper accompanying koen's color description software. Colour descriptions around salient points (or other point sampling technique) are calculated. So for each image there is a list of descriptor vectors (one vector for each sampled point). This list can vary in length. In order to turn this into a fixed length feature vector, the descriptors are compared to a "codebook" which contains a fixed number of elemens. (bag of words model). Each descriptor is assigned to one of the codebook elements, creating a histogram which shows the amount of descriptors in each codebook element (This codebook needs to be created by us using training data?). Finally this histogram is normalized and used as the feature vector.   NOTE: not sure what kind of descriptors we should use for this (they all have different invariance properties, so depends on what kind of invariance we want). 
 * Can't run koen's code though. Seems that its not compiled for my OS

==02/06==
 * Had initial meeting about the project and tasks at Science Park
 * My task is the extraction of statistical features
 * Installed ImageJ and looked at some of the features
 * Created research log

==01/06==
 * Project proposal presentation



= Possible features =

  * Avg Intensity / avg Hue
  * Weibull fit parameters
  * Number of edges / edge ratio
  * Corner detection 
  * Colour Histograms
  * Codebook vector produced by koen's code

= Implemented features =
 * edge/non-edge ratio (now with default parameters) (about 10-50 per second)
 * avg hue, avg saturation & avg Intensity (about 10-50 per second)
 * number of faces in image (processes about 3 images per second)
 * number of corners in an image (5-10 per second)
 